{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation, LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "documents = [[\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "[\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "[\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "[\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "[\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "[\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "[\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "[\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "[\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "[\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "[\"statistics\", \"R\", \"statsmodels\"],\n",
    "[\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "[\"pandas\", \"R\", \"Python\"],\n",
    "[\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "[\"libsvm\", \"regression\", \"support vector machines\"]]\n",
    "random.seed(0)\n",
    "# 총 문서의 수\n",
    "D = len(documents)\n",
    "\n",
    "# topic 수 지정\n",
    "K=4\n",
    "\n",
    "# 각 단어를 임의의 토픽에 랜덤 배정\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "for document in documents]\n",
    "\n",
    "# 각 토픽이 각 문서에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 문서를 의미\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "\n",
    "# 각 단어가 각 토픽에 할당되는 횟수\n",
    "# Counter로 구성된 리스트\n",
    "# 각 Counter는 각 토픽을 의미\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# 각 토픽에 할당되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# 각 문서에 포함되는 총 단어수\n",
    "# 숫자로 구성된 리스트\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = list(map(len, documents))\n",
    "\n",
    "# 단어 종류의 수\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "\n",
    "# 총 문서의 수\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) / (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) / (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    # i를 weights[i] / sum(weights)\n",
    "    # 확률로 반환\n",
    "    total = sum(weights)\n",
    "    # 0과 total 사이를 균일하게 선택\n",
    "    rnd = total * random.random()\n",
    "    # 아래 식을 만족하는 가장 작은 i를 반환\n",
    "    # weights[0] + ... + weights[i] >= rnd\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같이 랜덤 초기화한 상태에서\n",
    "# AB를 구하는 데 필요한 숫자를 세어봄\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'토픽-단어' 와 '문서-토픽' 에 대한 결합확률분포(unknown)로부터 표본을 얻음\n",
    "\n",
    "---\n",
    "### Gibbs Sampler (깁스 샘플링)\n",
    "\n",
    "깁스 샘플링은 다음 상태를 뽑을 때 한꺼번에 모든 변수(차원)의 값을 정하는 것이 아니라 변수 하나씩 값을 선택한다.\n",
    "\n",
    "다른 모든 변수는 현재값으로 고정되어 있다고 가정했을때, 이 변수가 가질 수 있는 값의 조건부 분포로부터 하나를 샘플링하는 것이다.\n",
    "\n",
    "이 과정을 모든 변수에 대해서 반복하면 하나의 새로운 상태가 만들어진다.\n",
    "\n",
    "즉, 새로운 Markov Chain이 형성 되는 것이다.\n",
    "\n",
    "⇒ 최종적으로, 우리는 새롭게 형성된 마코프 체인을 통해서 sample을 얻게 된다.\n",
    "\n",
    "---\n",
    " \n",
    "iteration : 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            # 깁스 샘플링 수행을 위해\n",
    "            # 샘플링 대상 word와 topic을 제외하고 세어봄\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # 깁스 샘플링 대상 word와 topic을 제외한 \n",
    "            # 말뭉치 모든 word의 topic 정보를 토대로\n",
    "            # 샘플링 대상 word의 새로운 topic을 선택\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # 샘플링 대상 word의 새로운 topic을 반영해 \n",
    "            # 말뭉치 정보 업데이트\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference 결과 첫번째 문서의 토픽 비중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 0, 0: 7, 2: 0, 1: 0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 토픽의 단어 비중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'HBase': 1,\n",
       "         'scikit-learn': 0,\n",
       "         'pandas': 0,\n",
       "         'R': 0,\n",
       "         'regression': 0,\n",
       "         'Java': 3,\n",
       "         'C++': 1,\n",
       "         'Haskell': 0,\n",
       "         'statistics': 0,\n",
       "         'artificial intelligence': 0,\n",
       "         'Hadoop': 2,\n",
       "         'Big Data': 3,\n",
       "         'statsmodels': 0,\n",
       "         'libsvm': 0,\n",
       "         'Spark': 1,\n",
       "         'Storm': 1,\n",
       "         'programming languages': 1,\n",
       "         'machine learning': 0,\n",
       "         'MapReduce': 1,\n",
       "         'scipy': 0,\n",
       "         'numpy': 0,\n",
       "         'support vector machines': 0,\n",
       "         'Cassandra': 1,\n",
       "         'deep learning': 1,\n",
       "         'decision trees': 0,\n",
       "         'neural networks': 0,\n",
       "         'databases': 0,\n",
       "         'probability': 0,\n",
       "         'theory': 0,\n",
       "         'NoSQL': 0,\n",
       "         'Mahout': 0,\n",
       "         'mathematics': 0,\n",
       "         'Postgres': 0,\n",
       "         'Python': 0,\n",
       "         'MySQL': 0,\n",
       "         'MongoDB': 0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_counts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 토픽은 대략 ‘Big Data’에 해당하는 주제인 것으로 추론"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
